\documentclass[a4paper, 11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}

\usepackage{geometry}
\usepackage{mathtools, amssymb}

\usepackage[vcentermath]{youngtab}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{Def}{Definizione}[section]
\newtheorem*{Oss}{Osservazione}
\theoremstyle{plain}
\newtheorem{Lemma}[Def]{Lemma}
\newtheorem{Prop}[Def]{Proposizione}
\newtheorem{Teo}[Def]{Teorema}
\newtheorem{Cor}[Def]{Corollario}

\usepackage[shortlabels]{enumitem}
\setlist[enumerate, 1]{label = (\roman*)}


\DeclarePairedDelimiter{\ang}{\langle}{\rangle}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\zero}{\mathbf{0}}
\newcommand{\eset}{\varnothing}
\newcommand{\restr}[2]{{#1}_{|_{#2}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\MM}{\mathfrak{M}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\deff}{\coloneqq}
\newcommand{\CC}{\mathcal{C}}
\DeclareMathOperator{\Mult}{Mult}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Tan}{Tan}
\DeclareMathOperator{\Gr}{Gr}
\DeclareMathOperator{\Seg}{Seg}
\DeclareMathOperator{\rk}{rk}
\DeclareMathOperator{\brk}{brk}
\DeclareMathOperator{\mrk}{mrk}
\DeclareMathOperator{\minrkop}{minrk}
\newcommand{\minrk}[4]{\minrkop(#1 \!\!\!\mod #2\ #3\ #4)}

\usepackage{booktabs}

\usepackage[autostyle, italian=guillemets]{csquotes}
\usepackage[backend=biber, style=numeric, hyperref, giveninits=true]{biblatex}
	\addbibresource{TensorSpaces.bib}
	\DeclareFieldFormat{url}{\url{#1}}

\usepackage{tikz}
\usepackage{hyperref}
\hypersetup{hidelinks}

\title{Introduction to Tensor Spaces\\ Appunti del Corso}
\author{Mirko Torresani}
\begin{document}
	\maketitle
\section{Fatti Introduttivi}
Per noi gli spazi vettoriali saranno di dimensione finita, con campo base $\C$. 
\begin{Def}
	Il prodotto tensoriale $V_1 \otimes \dots \otimes V_n$ è definito come lo spazio $\Mult(V_1, \dots, V_n;\C)$.
\end{Def}

\begin{Def}
	Dato un tensore $f \in V \otimes W$, il suo rango $\rk f$ è 
	\[
		\rk f = \min\{s \mid f = \sum_{i = 1}^s v_i \otimes w_i\}\,.
	\]
\end{Def}
\begin{Prop}
	Il rango $\rk f$ è equivalentemente definibile come
	\begin{enumerate}
		\item il rango del morfismo $V^* \to V$ associato a $f$;
		\item posto $f = \sum c_{ij} v_i \otimes w_j$, con $(v_i)_i$ e $(w_j)_j$ rispettive basi, il rango di $f$ è il rango della matrice $(c_{ij})_{i,j}$.
	\end{enumerate}
\end{Prop}

Nel caso in cui abbiamo un prodotto tensore di più spazi, le cose si complicano.
\begin{Def}
	Dato un elemento $f \in V_1\otimes \dots\otimes V_d$, il rango $\rk f$ è definito come
	\[
		\rk f = \min\{s \mid f = \sum_{j = 1}^s v_{j,1} \otimes \dots \otimes  v_{j,d}\}
	\]
\end{Def}

Un argomento, storicamente molto importate, riguarda il \emph{calcolo del rango tensoriale}. Per una sua prima trattazione introduciamo la seguente notazione: se $f$ è un vettore in $V_1 \otimes \dots \otimes V_d$, allora $f$ induce mappe 
\[
	f_k \colon V_k^\ast \to \bigotimes_{i \neq k} V_i \quad f_k^\dagger \colon \bigotimes_{i \neq k} V_i^* \to V_k
\]
per ogni $k$.
\begin{Def}
	Un tensore $f \in V_1 \otimes \dots \otimes V_d$ si dice $V_i$-conciso, o $i$-conciso, se $f_i$.
\end{Def}
\begin{Def}
	Il multi-rango di $f$ è definito come
	\[
		\mrk f = (\rk f_1, \dots, \rk f_d) \eqqcolon (r_1, \dots, r_d) \,,
	\]
	dove $\rk f_k$ è il rango di $f_k$ come mappa lineare (o equivalentemente il rango della mappa trasporta $f_k^\dagger$).
\end{Def}

Per il resto della trattazione useremo la \emph{notazione di Einstein: quando lo stesso indice compare come pedice e apice, allora viene intesa una sommatoria rispetto a quell'indice, se non diversamente indicato}.
\begin{Prop}
	Sia $f$ un tensore, allora 
	\[
		\max_i r_i \le \rk f \le \min_i \prod_{j \neq i} r_j
	\]
\end{Prop}
\begin{proof}
	Sia $r$ il rango di $f$, e poniamo 
	\[
		f = \sum_{i=1}^r v_{1,i}\otimes\dots\otimes v_{d,i}\,.
	\]
	L'immagine della funzione trasporta $f_k^\dagger$, da $\bigotimes_{i \neq k}V_i^\ast \to V_k$, è contenuta nel generato $\ang{v_{k,1}, \dots, v_{k,r}}$, e quindi l'immagine ha dimensione ha al più dimensione $r$.
	
	Se $\{u_{i,1}, \dots, u_{i,r_i}\}$ è una base per l'immagine di $f_i^\dagger$, allora $f$ si può scrivere come
	\[
		f = \alpha^{j_1, \dots, j_d}\,u_{1,j_1}\otimes \dots \otimes u_{d,j_d}
	\]
	e per ogni $k$ 
	\[
		f = u_{1,j_1}\otimes \dots \otimes u_{k-1, j_{k-1}}\otimes \left[\sum_{j_k=1}^{r_k}\alpha^{j_1,\dots, j_d}u_{k,j_k}\right]\otimes u_{k+1, j_{k+1}}\otimes \dots \otimes u_{d,j_d}\,.
	\]
	Conseguentemente per ogni $k$, il rango $r$ è al più $\prod_{i \neq k}r_i$.
\end{proof}
\begin{Cor}
	Se $\rk f = 1$, allora $\rk f_k = 1$ per ogni $k$.
\end{Cor}
\begin{Cor}
	Fissato un certo $k$, se $r_j = 1$ per ogni $j \neq k$ allora $\rk f_k = \rk f = 1$.
\end{Cor}
\begin{Prop}
	Sia $f$ un tensore 1-conciso, tale che $r_1 \ge \dots \ge r_d$ e che $\rk f = r_1$. Allora $f_1(V_1^*)$ è generato precisamente da $r_1$ tensori indecomponibili in $V_2 \otimes \dots \otimes V_d$.
\end{Prop}
\begin{proof}
	Sappiamo che $f = \sum_{i =1}^{r_1}v_{1,i}\otimes\dots\otimes v_{d,i}$ via vettori arbitrari. Conseguentemente, l'immagine di
	\[
		f_1^\dagger \colon \bigotimes_{i > 1} V_i^\ast \to V_1\,
	\]
	è generata da $\{v_{1,1}, \dots, v_{1,r_1}\}$. Siccome il rango di $f_1$, e quindi quello di $f_1^\dagger$, è per ipotesi $r_1$, quei vettori devono essere necessariamente indipendenti. Inoltre, per ipotesi, il tensore $f$ è 1-conciso, e quindi $f_1$ è iniettivo. In definitiva, $\dim V_1^* = \dim V_1 = r_1$ e $\{v_{1,1}, \dots, v_{1,r_1}\}$ formano una base di $V_1$. 
	
	Consideriamo quindi la base duale $\{v_1^1, \dots, v_1^{r_1}\}$ di $V_1^*$. Per costruzione
	\[
		f(V_1^*) = \ang{f(v_1^1), \dots, f(v_1^{r_1})} = \ang{v_{2,i}\otimes\dots\otimes v_{d,i}}_{i = 1,\dots, r_1}\,. \qedhere
	\]
\end{proof}

Come non-esempio consideriamo $\C^2 \otimes \C^2 \otimes \C^2$, ed il tensore
\[
	f \deff e_0 \otimes e_0 \otimes e_1 + e_0 \otimes e_1 \otimes e_0 \otimes e_1 \otimes e_0 \otimes e_0 \,.
\]
Si può osservare che in effetti è 1-conciso, e che 
\[
	f(V_1^\ast) = \ang{e_0\otimes e_1 + e_1 \otimes e_0, e_0 \otimes e_0}\,.
\]
Tuttavia quest'ultima espressione non può essere ricondotta ad uno span di tensori indecomponibili. Inoltre, $\mrk f$ è $(2,2,2)$. Conseguentemente, $\rk f = 3$ come ci si può immaginare.

\begin{Prop}
	Sia $f \in V_1 \otimes\dots\otimes 	V_d$. Il rango di $f$ coincide col minimo numero di elementi indecomponibili necessari per generare uno spazio che contiene $f_1(V_1^\ast)$.
\end{Prop}
\begin{proof}
	Se $r$ è il rango di $f$, allora $f$ si scrive come $\sum_{i = 1}^r v_{1,i}\otimes \dots \otimes v_{d,i}$ e conseguentemente $f_1(V_1^\ast)$ è contenuto in $\ang{v_{2,i}\otimes\dots\otimes v_{d,i}}_{i=1}^r$.
	
	D'altra parte, supponiamo che $f_1(V_1^*)$ sia contenuto in $\ang{v_{2,i}\otimes\dots\otimes v_{d,i}}_{i=1}^r$. Fissiamo una base $\{v_{1,1}, \dots, v_{1,m}\}$ di $V_1$, ed una conseguente base duale. Allora
	\[
		f_1(v_1^k) = \alpha^{k,i}\,v_{2,i}\otimes\dots\otimes v_{d,i}\quad 1\le k\le r,
	\]
	e 
	\[
		f = \alpha^{k,i}\,v_{1,k}\otimes v_{2,i}\otimes\dots\otimes v_{d,i}\,.\qedhere
	\]
\end{proof}

Consideriamo ora il caso in cui tensoriamo solo per \emph{tre} spazi.
\begin{Def}
	Siano $A$, $B$ e $C$ tre spazi vettoriali su $\C$. Inoltre sia $\{a_1, \dots, a_n\}$ una base di $A$, e sia $V$ un sottospazio di $B \otimes C$ con base $\{v_1, \dots, v_m\}$. Una \emph{modificazione} di $f \in A \otimes B \otimes C$ è una somma della forma
	\[
		f + \sum_{i,j} a_i \otimes v_j\,.
	\]
	Analogo per $V$ in $A \otimes B$ o in $A \otimes C$.
\end{Def}
\begin{Def}
	Dato $V_1 \subseteq B \otimes C$, $V_2 \subseteq A \otimes C$ e $V_3 \subseteq A \otimes B$ il \emph{rango minimale modulo tre sottospazi $V_1$, $V_2$ e $V_3$} è 
	\[
		\minrk{f}{V_1}{V_2}{V_3} \deff \min \{\rk \tilde{f} \mid \tilde{f} \text{ modificazione ottenuta da un singolo } V_i\}\,.
	\]
\end{Def}
\begin{Prop}
	Sia $f \in A \otimes B \otimes C$ un tensore conciso di rango $r$, e poniamo $f = \sum_{k=1}^m g_k \otimes c_k$ con $g_i \in A \otimes B$ e $\{c_1, \dots, c_{m}\}$ una base di $C$. Se $g_1 \neq 0$, esistono constanti $\lambda_2, \dots, \lambda_m \in \C$ tali che 
	\[
		\hat{f} = \sum_{j=2}^{m}(g_j - \lambda_j g_1) \otimes  c_j \in A \otimes B \otimes (c_1^\bot)^*
	\]
	ha rango al più $r-1$. Se $\rk g_1 = 1$, allora $\hat{f}$ ha rango almeno $r-1$ qualunque siano le costanti.
\end{Prop}
\begin{proof}
	Sappiamo che esistono $h_1, \dots, h_r$, tensori di rango 1 in $A \otimes B$, che generano uno spazio contenente $f_3(C^*)$. Quindi
	\[
		g_j = \alpha^{j,t}\,h_t \in A \otimes B.
	\]
	
	Conseguentemente
	\[
		f = \alpha^{j,t}\, h_t \otimes  c_j\,.
	\]
	Possiamo assumere, senza perdita di generalità, $\alpha^{1,1} \neq 0$, e porre $\lambda_j \deff \alpha^{j,1}/\alpha^{1,1}$. Otteniamo quindi
	\[
		\begin{split}
			\hat{f} &= \sum_{j=2}^m(g_j - \lambda_jg_1)\otimes c_j\\
			&= \sum_{j=2}^m \left[\alpha^{j,t}h_t - \frac{\alpha^{j,1}}{\alpha^{1,1}}\,\alpha^{1,t}h_t \right]\otimes c_j\\
			&= \sum_{j=2}^m  \left[\sum_{t=2}^r\left( \alpha^{j,t} - \frac{\alpha^{j,1}\alpha^{1,t}}{\alpha^{1,1}}\right)h_t  \right] \otimes c_j
		\end{split}\,.
	\]
	Ergo $\hat{f}_3(c_1^\bot)$ è contenuto in $\ang{h_2, \dots, h_r}$, che uno span di tensori di rango 1. Quindi $\hat{f}$ ha rango al più $r-1$.
	
	Se il rango di $g_1 \in A \otimes B$ è 1, allora possiamo tranquillamente porre $h_1 = g_1$. In questo caso $\alpha^{1,t}= 0$ per ogni $t > 1$ e $\hat{f}$ assume la forma seguente, \emph{indipendentemente dalle costanti $\lambda_j \in \C$}:
	\[
		\hat{f} = \sum_{j=2}^m \sum_{t=2}^r\alpha^{j,t}\,h_t\otimes c_j = \sum_{t = 2}^th_t \otimes \left[\sum_{j=2}^m \alpha^{j,t}\, c_j\right]\,.
	\]
	
	Ma questo implica che $\hat{f}_3(c_1^\bot) $ coincide con $\ang{h_2, \dots, h_r}$, da cui
	\[
		\rk \hat{f} \ge \rk\hat{f}_3 = r-1\,.\qedhere
	\]
\end{proof}
\begin{Cor}
	Sia $f \in A \otimes B \otimes  C$, e sia $f$ un tensore $C$-conciso. Fissato un sottospazio $W$ di $C^*$, allora
	\[
		\rk f \ge \minrk{f}{0}{0}{f_3(W)} + \dim W\,,
	\]
	e l'uguaglianza si ottiene se $f(W)$ è generato da tensori di rango 1.
\end{Cor}
\begin{proof}
	Applichiamo la proposizione precedente per un numero di volte pari a $\dim W$.
\end{proof}
\begin{Cor}
	Se $f \in A \otimes B \otimes C$ è conciso, e $U \subseteq A^*$, $V \subseteq B^*$, $W \subseteq C^*$ sono sottospazi, allora
	\[
		\rk f \ge \minrk{f}{f(U)}{f(V)}{f(W)} + \dim U + \dim V + \dim W\,,
	\]
	e se $f(U)$, $f(V)$, $f(W)$ sono generati da tensori dai rango 1, vale l'uguaglianza.
\end{Cor}

\section{Algebre Tensoriali}

Parliamo brevemente di algebre tensoriali.
\begin{Def}
	Dato un gruppo $G$, un $G$-modulo è, in questo contesto, un $\C[G]$-modulo nel senso dell'algebra commutativa.
\end{Def}
\begin{Def}
	Se $G$ agisce su un $\C$-spazio $V$ e $W$ (tramite un morfismo $G \underset{\rho}{\to} GL(V)$)
	\begin{enumerate}
		\item $G$ agisce su $V^*$ via $\rho^*(g) = [\rho(g)^{-1}]^\dagger$;
		\item $G$ agisce su $V \oplus W$ via $g \cdot (v,w) = (g\cdot v, g\cdot w)$;
		\item $G$ agisce su $V \otimes W$ via $g \cdot (v\otimes w) = (g\cdot v)\otimes (g\cdot w)$.
	\end{enumerate}
\end{Def}
\begin{Def}
	L'algebra tensoriale $(TV, \otimes)$ è definita come
	\[
		TV \deff \bigoplus_{d \ge 0} V^{\otimes d}\,.
	\]
\end{Def}

Vogliamo definire \emph{l'algebra simmetrica}. 
\begin{Def}
	I $d$-tensori simmetrici sono 
	\[
		S^d V \deff \{\alpha \in V^{\otimes d} \mid \sigma \cdot \alpha = \alpha\ \forall \sigma \in S_d \}\,,
	\]
	e l'algebra simmetrica è
	\[
		SV \deff \bigoplus_{d \ge 0} S^{d}V\,.
	\]
\end{Def}

Definiamo ora la \emph{proiezione simmetrica $\pi_S$} da $TV$ in $SV$:
\[
	\pi_S(v_1 \otimes\dots\otimes v_d) = \frac{1}{d!}\sum_{\sigma \in S_d}\sigma \cdot (v_1\otimes\dots\otimes v_d)\,.
\]
\begin{Prop}
	Lo spazio $S^dV$ è generato dall'insieme $\{v^{\otimes d} \mid v \in V\}$.
\end{Prop}
\begin{proof}
	Basta osservare che 
	\[
		\sum_{\sigma \in S_d} v_{\sigma(1)}\otimes \dots \otimes v_{\sigma(d)} = \sum_{\mathclap{\substack{I \subseteq \{1,\dots, d\}\\I \neq \eset}
		}} (-1)^{d -\abs{I}} \bigg[\sum_{i \in I}v_i\bigg]^{\otimes d}\,,
	\]
	e che per ogni $\alpha \in S^dV$ la somma precedente coincide con $d!\cdot \alpha$.
\end{proof}

L'algebra $SV$ risulta effettivamente un algebra, grazie all'introduzione del \emph{prodotto simmetrico $\odot$} su $SV$ come
\[
	\alpha \odot \beta = \pi_S(\alpha\otimes\beta)	\,.
\]

\begin{Oss}
	Se $v_1, \dots, v_n$ è una base di $V$, una base di $S^dV$ è data da
	\[
		\mathcal{B}_{S^dV} = \{v_{j_1}\odot\dots\odot v_{j_d}\}_{1\le j_1 \le \dots \le j_d\le n}
	\]
	e quindi
	\[
		\dim S^dV = \binom{n+d-1}{d}\,.
	\]
\end{Oss}

In seguito sarà molto importante parlare di \emph{decomposizione di tensori}. Un esempio in quella direzione viene ai prossimi risultati.
\begin{Prop}
	Dato un tensore $f \in S^2V$ di rango $r$, esso ammette una decomposizione della forma
	\[
		f = \sum_{i=1}^r v_i \otimes v_i\,.
	\]
\end{Prop}
\begin{Prop}
	La rappresentazione di $GL(V)$ sullo spazio vettoriale $S^2V$ è irriducibile.
	\begin{proof}
		Sia $W \subseteq S^2V$ un $GL(V)$-sottomodulo contenente un tensore $f$ non nullo. Sicuramente possiamo scrivere
		\[
			f = \sum_{i=1}^rv_i \otimes v_i \quad v_i \in V, \lambda_i \in \C\,.
		\]
		con $v_1,\dots, v_r$ indipendenti.
		
		Sia un morfismo $g \in GL(V)$ per cui $g(v_1) = 2v_1$ e $g(v_i) = v_i$ per ogni $i > 1$. Allora vale che 
		\[
			W \ni \frac{1}{3}(g \cdot f - f) = v_1\otimes v_1\,,
		\]
		e quindi 
		\[
			S^2V = \ang{(g\cdot v_1) \otimes (g\cdot v_1)}_{g \in GL(V)} \subseteq W\qedhere
		\]
	\end{proof}
\end{Prop}

Possiamo analogamente definire un \emph{rango simmetrico}.
\begin{Def}
	Il rango simmetrico di $f \in S^dV$ è
	\[
		\rk_S f \deff \min\{r \in \N \mid f = v_1^{\otimes d} + \dots + v_r^{\otimes d}\}\,.
	\]
\end{Def}
Sicuramente $\rk f \le \rk_S f$, e vale l'uguaglianza per $d = 2$. È una congettura se sono uguali, detta \emph{congettura di Comon}. Nel 2018 Shitov \cite{Shitov2018} ha pensato di trovare un controesempio, smentito da sé stesso nel 2024 \cite{Draisma2024}.

\begin{Prop}
	
	Posto $\C[V]$ l'algebra delle funzioni $V \to \C$ generata da $V^*$, lo spazio $S^dV^*$ è isomorfo a $\C[V]_d \simeq \C[x_1, \dots, x_n]_d$.
\end{Prop}
\begin{proof}
	La mappa che funziona è 
	\[
		\Phi \colon S^dV^* \to \C[V]_d\,, \ \phi \mapsto f_\phi\,,
	\]
	con
	\[
			f_\phi(v) = \phi(v,\dots, v)\,. \qedhere
	\]
\end{proof}
\begin{Def}[Waring rank]
	Per ogni $f \in \C[x_1, \dots, x_n]_d$ il rango di Waring è 
	\[
		\rk_S f = \min\{r \in \N \mid f = l_1^d + \dots + l_r^d, \ l_i \text{ forma lineare}\}\,.
	\]
\end{Def}

\begin{Def}
	I $d$-tensori antisimmetrici sono 
	\[
	\Lambda^d V \deff \{\alpha \in V^{\otimes d} \mid \sigma \cdot \alpha =\sgn(\sigma)\, \alpha\ \forall \sigma \in S_d \}\,,
	\]
	e l'algebra antisimmetrica è
	\[
	\Lambda V \deff \bigoplus_{d \ge 0} \Lambda^{d}V\,.
	\]
\end{Def}

Definiamo la proiezione antisimmetrica $\pi_\Lambda$ come
\[
	\pi_\Lambda(v_1 \otimes\dots\otimes v_d) = \frac{1}{d!}\sum_{\sigma \in S_d}\sgn(\sigma)\,v_{\sigma(1)}\otimes\dots\otimes v_{\sigma(d)}\,.
\]
Analogamente a quanto fatto prima definiamo il prodotto antisimmetrico (o \emph{wedge}) come
\[
	\alpha \wedge \beta \deff \pi_\Lambda(\alpha\otimes\beta)\,.
\]
\begin{Prop}
	Un insieme finito $v_1, \dots, v_d$ è linearmente indipendente se e solo se 
	\[
		v_1\wedge \dots \wedge v_d = 0\,.
	\]
\end{Prop}
\begin{Cor}
	Una base $\mathcal{B}_{\Lambda^dV}$ di $\Lambda^dV$ è 
	\[
		\mathcal{B}_{\Lambda^dV} = \{v_{j_1}\wedge\dots\wedge v_{j_d}\}_{1\le j_1 < \dots< j_d\le n}
	\]
	e quindi
	\[
		\dim \Lambda^dV = \binom{n}{d}\,.
	\]
\end{Cor}
\begin{Prop}
	Lo spazio $\Lambda^2V$ è un $GL(V)$-modulo irriducibile.
\end{Prop}

\section{Decomposizione di Tensori}

Dati due spazi $A$, $B$, lo spazio $G \deff GL(A) \times (B)$ è incluso in $GL(A\otimes B)$. Dei teoremi di semplice decomposizione sono dati dai seguenti.
\begin{Prop}
	Lo spazio $S^2(A\otimes B)$ si $G$-decompone come
	\[
		S^2(A \otimes B) = (S^2A \otimes S^2B) \oplus (\Lambda^2A \otimes \Lambda^2B)\,.
	\]
	Ed allo stesso modo $\Lambda^2(A\otimes B)$ si $G$-decompone come
	\[
		\Lambda^2(A\otimes B) = (S^2A \otimes \Lambda^2B) \oplus (S^2A \otimes \Lambda^2B)\,.
	\]
\end{Prop} 

Gli elementi di $(\C^2)^{\otimes 3}$ hanno le orbite secondo $(GL_2(\C))^3$ che seguono la seguente tabella:
\[
\begin{array}{cccccc}
	\toprule
	\text{orbita} & r_1 & r_2 & r_3 &  \rk f & \text{rappresentante}\\
	\midrule
	A & 1 & 1 & 1 & 1 & a_0\otimes b_0 \otimes c_0 \\
	B_1 & 1 & 2 & 2 & 2 & a_0\otimes b_0 \otimes c_0 + a_0\otimes b_1 \otimes c_1\\
	B_2 & 2 & 1 & 2 & 2 & a_0\otimes b_0 \otimes c_0 + a_1\otimes b_0 \otimes c_1\\
	B_3 & 2 & 2 & 1 & 2 & a_0\otimes b_0 \otimes c_0 + a_1\otimes b_1 \otimes c_0\\
	W & 2 & 2 & 2 & 3 & a_0\otimes b_1 \otimes c_1 + a_1\otimes b_0 \otimes c_1 + a_1\otimes b_1 \otimes c_0\\
	G & 2 & 2 & 2 & 2 & a_0\otimes b_0 \otimes c_0 + a_1\otimes b_1 \otimes c_1 \\
	\bottomrule
\end{array}
\]
\section{Varietà Algebriche Tensoriali}
\begin{Def}
	Sia $Z \subseteq \PP V$ un sottoinsieme dello spazio proiettivo su $V$. Il \emph{cono affine} è $\hat{V} \deff \pi^{-1}(Z)$, con $\pi$ la proiezione proiettiva.
\end{Def}
\begin{Def}
	Se $X$ l'insieme di zeri comuni di $S \subseteq S^\bullet V^\ast$, allora poniamo $X \deff Z(S)$.
\end{Def}

\begin{Def}
	Viceversa, dato $A \subseteq \PP V$, 
	\[
		I(A) \deff \{F \in S^\bullet V^\ast \mid F(a) = 0\ \forall a \in \hat{A}\}
	\]
	è \emph{l'ideale di $A$}.
\end{Def}
\begin{Def}
	L'embedding di Segre è dato da
	\[
	\begin{split}
		\Seg \colon \PP A \times \PP B &\to \PP (A \otimes B)\\
		([a], [b]) &\mapsto [a \otimes b]
	\end{split}
	\]
\end{Def}
L'immagine è data dalla proiezione delle matrici $\dim A \times \dim B$ di rango $1$, cioè dal luogo di zeri di $\Lambda^2A^\ast \otimes \Lambda^2B^\ast \subseteq S^2(A \otimes B)$.
\begin{Prop}
	In generale l'analoga mappa da $\PP A_1 \times \dots \times \PP A_n $ a $\PP(A_1 \otimes\dots\otimes A_n)$ dà come immagine un'insieme chiuso.
\end{Prop}
\begin{Def}
	La $d$-mappa di Veronese è
	\[
	\begin{split}
		v_d \colon \PP V &\to \PP(S^dV) \\
		[a] &\mapsto [a^{\otimes d}]
	\end{split}
	\]
\end{Def}

L'immagine è costituita da $\Seg((\PP V)^n) \cap \PP(S^d V)$, e quindi è una varietà proiettiva.

\begin{Def}
	Data una mappa $f$ da $V$ in sé, $f^{\wedge m}$ è la naturale endomorfismo di $\Lambda^m V$. Se $m = \dim V$, $f^{\wedge m}$ è la moltiplicazione per $\det(f)$.
\end{Def}
\begin{Def}
	La Grasmanniana è 
	\[
		\Gr(r,V) \deff \{[v_1 \wedge \dots \wedge v_r] \mid v_f \in V\} \subseteq \PP(\Lambda^r V).
	\]
\end{Def}
Osserviamo che nel proiettivo un tale prodotto wedge è insensibile a cambi di base del sottospazio generato. La Grassmaniana parametrizza quindi i sottospazi 

\begin{Def}
	Per ogni $\phi \in V^\ast$ e $v \in V$, definiamo $\phi \,\lrcorner \,v \deff \phi(v)$. Per induzione se $\phi \in V^\ast$, $v \in V$ e $f \in \Lambda^k V$ imponiamo
	\[
		\phi\,\lrcorner\,(v \wedge f) \deff (\phi \,\lrcorner\, v) \wedge f - v \wedge (\phi \,\lrcorner\, f)\,.
	\]
	Infine imponiamo $(\phi \wedge g)\,\lrcorner\, f \deff \phi\,\lrcorner\,(g\, \lrcorner\, f)$.
\end{Def}
\begin{Prop}
	$f \in \Lambda^r V$ può essere scritta come un prodotto wedge $w_1 \wedge\dots\wedge w_r$ se e solo se 
	\[
		(\psi \,\lrcorner\,f) \wedge f = 0\ \forall \psi \in \Lambda^{r-1}V^*
	\]
\end{Prop}
In particolare, se 
\[
	f = p_{i_1\dots i_r}v^{i_1} \wedge \dots \wedge v^{i_r}
\] 
allora l'equazione diventa
\[
	\sum_{k=1}^{r+1}(-1)^kp_{i_1\dots i_{r-1}j_k}\,p_{j_1\dots j_{k-1}j_{k+1}\dots j_{r+1}} = 0\,,
\]
per ogni scelta di multiindici $(i_1, \dots, i_k)$ e $(j_1, \dots, j_k)$. Avendo ottenuto una equazione polinomiale, $\Gr(r,V)$ è una varietà proiettiva.

Parliamo ora di spazi tangenti. 
\begin{Def}
	Sia $M \subseteq V$ un sottoinsieme e $v \in V$. Allora
	\[
		\hat{T}_vM \deff \Big\{\frac{d\gamma}{dt}\Big|_{t = 0} \mid \gamma \colon \C \to M \text{ curva liscia}\Big\}
	\]
	è lo spazio tangente.
\end{Def}
Osserviamo che lo spazio tangente su $v$ o su $\lambda v$ rimane invariato per $\lambda \in \C^*$.

\begin{Def}
	Sia $X \subseteq \PP V$ una varietà proiettiva. Un punto $v \in X$ si dice liscio se esiste un insieme aperto $U$ (di Zarinksi) su cui lo spazio tangente $\hat{T}_w X$ ha la stessa dimensione per ogni $w \in U$. L'insieme dei punti singolari è un chiuso proiettivo.
\end{Def}

Poniamo quindi 
\[
	\dim X \deff \dim (\hat{T}_v X)-1
\]
con $v$ un punto liscio.

\begin{Prop}
	Vale che
	\[
		\hat{T}_{v_1\otimes \dots \otimes v_d} \Seg(\PP V_1 \times \dots \times \PP V_d) = \sum_{j =1}^d
 v_1 \otimes \dots \otimes v_{j-1}\otimes V_j\otimes v_{j+1} \otimes \dots \otimes v_d\,.
 	\]
\end{Prop}
\begin{Prop}
	Analogamente vale che
	\[
		\hat{T}_{[v^{\otimes d}]} v_d(\PP V) = \{[v^{\otimes\, (d-1)}\otimes w] \mid w \in V\}\,.
	\]
\end{Prop}

Parliamo ora di orbite. Sulle nostre varietà proiettive facciamo agire $G \deff SL(V)$. È il rivestimento universale (di grado finito) di $PGL(V)$. 
\begin{Prop}[Borel]
	Se un'azione è algebrica, allora per ogni orbita $\mathcal{O}$ la sua chiusura è ancora $G$-invaiante e 
	\[
		\overline{\mathcal{O}} = \mathcal{O} \cup \{\text{orbite di dimensione minore}\}.
	\]
\end{Prop}
Conseguentemente, le orbite di dimensione minore sono chiuse, e se l'azione è irriducibile, essa è unica.

In questa ottica delle azioni, guardiamo alle tre varietà di prima.
\begin{enumerate}
	\item Dato l'embedding di Veronese $v_d$, esso è $G$-equivariante, e la sua immagine è \emph{l'orbita chiusa} per l'azione di $G$ su $\PP(S^dV)$. 
	
	\item Per quanto riguarda, l'embedding di Segre, l'immagine è l'orbita chiusa per l'azione di $SL(V_1) \times \dots\times SL(V_d)$.
	
	\item Possiamo anche considerare varietà di Segre-Veronese denntro 
	\[
		\PP(S^{a_1}V \otimes \dots \otimes S^{a_d}V)
	\]
	\item Le varietà di Grassmann \emph{proiettive}, indicate come $\Gr(\PP^k, \PP V)$, possiedono delle naturali coordinate di Plücker che le immergono dentro $\PP(\Lambda^{k+1} V)$.
\end{enumerate}

Parliamo ora di \emph{varietà secanti}. Iniziamo ora con delle definizioni.
\begin{Def}
	Siano $X$ e $Y$ sottoinsiemi di $\PP(W)$. Il Join è definito come
	\[
		J(X,Y) \deff \overline{\bigcup_{\substack{x \in X \\ y \in Y}} \ang{x,y}}
	\]
\end{Def}
La chiusura serve per prendere anche le tangenti come limite di secanti.


Se $X = Y = v_3(\PP\C^2)$, dato in coordinate come ($\dim_\C S^3\C^2 = 4$)
\[
	[x_0, x_1] \mapsto [x_0^3, 3x_0^2x_1, 3x_0x_1^2, x_1^3]\,,
\]
allora $X$ prende il nome di \emph{cubica gobba}. Studiamo ora l'azione di $SL_2$ su $\PP(S^3\C^2)$. Abbiamo tre orbite che sono 
\[
	X,\ \Tan(X) \setminus X,\ \PP(S^3\C^2) \setminus \Tan(X)
\]

Se $f \in S^3\C^2$, allora può essere pensato come un polinomio omogeneo cubico in $x_0$ e $x_1$. Su $\C$ ricade in tre casistiche
\begin{enumerate}
	\item $f$ una radice tripla, e appartiene a $X$.
	\item $f$ ha una radice singola ed una doppia, ed appartiene a $\Tan(X) \setminus X$;
	\item $f$ ha tre radici singole, ed appartiene a $\PP(S^3\C^2) \setminus \Tan(X)$.
\end{enumerate}

Può essere dimostrato che $X \cup (\PP(S^2\C^3)\setminus \Tan(X))$ coincide esattamente con l'unione delle secanti. Detto altrimenti, $J(X,X)$ è tutto lo spazio $\PP(S^3\C^2)$.

\begin{Def}
	La varietà secante è data da 
	\[
		\sigma_k(X) \deff J(X, \dots, X)\,.
	\]
\end{Def}
Abbiamo quindi una catena ascendente
\[
	X \subseteq \sigma_1(X) \subseteq \dots \subseteq \sigma_{p_0}(X) = \PP(W)\,.
\]

Se consideriamo $X$ come $\PP(V_1) \times \PP(V_2)$ dentro $\PP(V_1 \otimes V_2)$, allora
\[
	\sigma_r(X) = \{f \in \Hom(V_1^{\vee}, V_2) \mid \rk f \le r\}\,,
\]
in quanto ogni matrice di rango $r$ (in senso matriciale) può essere scritta come somma di matrici di rango 1.

Si può dimostrare che la chiusura di Zarinksky di 
\[
	\{f \in \Hom(V_1^\vee, V2) \mid \rk f = r\}
\]
coincide precisamente con $\sigma_r(X)$. Questo recupera il risultato di Borel.

Consideriamo ora il caso $X = v_d(\PP^1\C)$, con $d \ge 3$.

Per $d = 3$ siamo di fronte alla cubica gobba. Se supponiamo di proiettare questa cubica su un piano, usando un generico punto $p \notin X$ come fuoco, osserviamo che la proiezione ha un nodo precisamente se $p$ è in una secante, o ha una cuspide precisamente quando $p$ sta in una tangente. Ma una cubica in un piano può solo avere un nodo. Quindi se $p$ sta su una secante, essa è unica. Analogamente se $p \notin X$ sta su una tangente essa è unica. Si può provare, come già detto, che $\sigma_2(X) = \PP^3\C$\,.

Per $d = 4$ stiamo guardando l'immersione di $\PP^1\C$ in $\PP^4\C$. In questo caso abbiamo infinite $SL_2$-orbite, in quanto quattro radici non possono essere generalmente portate una dentro l'altra. La catena secante è della forma
\[
	X = \sigma_1(X) \subseteq \sigma_2(X) \subseteq \sigma_3(X) = \PP^4
\]
Capiamo l'equazione di $\sigma_2(X)$. Sia
\[
	S^2\C^{2\vee} = \{\alpha_0\partial_0^2 + 2 \alpha_1\partial_0\partial_1 + \alpha_2\partial_1^2\}
\]
e consideriamo la mappa
\[
\begin{split}
	\CC_f \colon S^2\C^{2\vee} &\to S^2\C^2 \\
	\partial &\mapsto \partial f
\end{split}
\]	
con $f \in S^4\C^2$.

Sia $\ell$ una forma lineare. Allora l'immagine di $\CC_{\ell^4}$ è $\C$-generata da $\ell^2$ ed ha dimensione 1. Inoltre,
\[
	\CC_{\lambda f + \mu g} = \lambda \CC_f + \mu \CC_g
\]
e quindi se $\ell_1$, $\ell_2$ sono due forme lineari, allora
\[
	\rk \CC_{\lambda_1 \ell_1^4 + \lambda_2 \ell_2^4} \le 2\,.
\]
Quindi la mappa precedente ha sempre determinante nullo, in quando $\dim S^2\C^2 = 3$.

Conseguentemente, la varietà $\sigma_2(X)$ è contenuta in $V(\det \CC_f)$. Inoltre vale l'uguaglianza in quanto $\det \CC_f$ è un polinomio cubico irriducibile.

Inoltre, $SL_2$ agisce su $S^4\C^2$, e quindi su 
\[
	\bigoplus_d S^d(S^4\C^2)\,.
\]
\begin{Prop}
	Il sottoanello
	\[
		\left[\bigoplus_d S^d(S^4\C^2)\right]^{SL_2} \subseteq \bigoplus_d S^d(S^4\C^2)
	\]
	è un anello polinomiale completo: $\C[I,J]$, con $\deg I = 2$ e $\deg J = 3$. Questo fatto (mi fido) è molto raro in teoria degli invarianti. Inoltre come varietà algebrica è $\PP^1\C$, oltre ad essere liscia. A meno di multipli scalari $J = \det \CC_f$. 
\end{Prop}

\begin{Lemma}
	$\dim \sigma_r(X) \le r\,\dim(X) + (r-1)$
\end{Lemma}
\begin{proof}
	Consideriamo 
	\[
		\sigma^r(X) \deff \{(x_1, \dots, x_r, y) \mid y \in \ang{x_1, \dots, x_r}\}
	\]
	dentro 
	\[
		X \times\dots\times X\times \PP W
	\]
	E consideriamo le due mappe
	\[
		X \times\dots\times X \overset{\pi_1}{\longleftarrow} \sigma^r(X) \overset{\pi_2}{\longrightarrow} \PP W \,.
	\]
	
	Notiamo che $\pi_2(\sigma^r(X)) = \sigma_r(X)$, e che $\pi_1$ è suriettiva con fibra generica $\PP^{r-1}\C$. Conseguentemente
	\[
	\begin{split}
			\dim \sigma_r(X) \le \dim \sigma^r(X) &= \dim(\text{Fibra}) + \dim(X \times\dots\times X) \\
			&= (r-1) + r\dim(X)\,. \qedhere
	\end{split}
	\]
\end{proof}
Si postula che valga l'uguaglianza.

\begin{Def}
	$X$ si dice essere $(h+1)$-difettivo se $\sigma_{h+1} X $ ha dimensione minore del minimo tra $X$ e la stima del teorema precedente.
\end{Def}
\begin{Lemma}[Terracini]
	Se $x_i \in X_i$ sono punti generici, e $p \in \ang{x_1, \dots, x_{h+1}}$ è generico, allora
	\[
		T_pJ(X_1, \dots, X_{h+1}) = \ang{T_{x_1}X_1, \dots, T_{x_{h+1}}X_{h+1}}\,.
	\]
\end{Lemma}

Sia ora $X \subseteq \PP^N$, e $p \in \PP^N$
\begin{Def}
	Il rango $\rk_X(p)$ è definito come
	\[
		\rk_X(p) = \min\{h+1 \mid p \in \ang{x_1, \dots, x_{h+1}}\}\,.
	\]
\end{Def}
\begin{Def}
Il rango di bordo $\brk_X(p)$ è definito come
\[
\brk_X(p) = \min\{h+1 \mid p \in \sigma_{h+1}(X)\}\,.
\]
\end{Def}
Ovviamente $\brk_X(p)$ è minore di $\rk_X(p)$.

Se $X = v_d(\PP V) = \{\ell^{\otimes d} \mid \ell \in \PP V\}$ in $\PP(S^d V)$, allora stiamo sostanzialmente guardando al rango simmetrico.

Se $X' = \PP V \times \dots \times \PP V$ dentro $\PP(V^{\otimes d})$, stiamo guardando al rango normale.

Siccome $v_d(\PP V)$ coincide con $X' \cap \PP(S^d V)$, allora ogni elemento di $\PP(S^dV)$ ha due ranghi. È una congettura se sono uguali.

Inoltre,  sicuramente vero che
\[
	\sigma_{h+1}(v_d(\PP V)) \subseteq \PP(S^d V) \cap \sigma_{h+1}(X')\,.
\]
È una congettura se valga l'uguaglianza.\vspace{0.5cm}

\noindent\fbox{\parbox{\textwidth}{Note terminate a causa della presenza di appunti del Prof.\ Ottaviani sulla teoria dell'apolarità}}
\section{Teoria dell'Apolarità}
Sia $K = \C$, e sia $V$ uno spazio vettoriale di dimensione $n+1$. Inoltre, siano 
\[
	R = K[x_0,\dots, x_n] \simeq S^\bullet V, \quad S = K[\partial_0, \dots, \partial_n] \simeq S^\bullet V^\vee 
\]
L'anello $R$ ha un unico ideale massimale omogeneo: $\MM = (\partial_0, \dots, \partial_n)$. Inoltre, $S$ agisce su $R$ additivamente tramite la ovvia azione che indichiamo con $\cdot$.

D'ora in poi useremo la notazione a multiindice.
\begin{Lemma}\label{Lemma:dualbas}
	Se $\alpha$ e $\beta$ sono multiindici tali che $\abs{\alpha} = \abs{\beta}$, allora
	\[
		\partial^\alpha \cdot x^\beta =
		\begin{cases}
			\alpha! & \alpha = \beta \\
			0 & \alpha \neq \beta
		\end{cases}
	\]
\end{Lemma}
\begin{Cor}
	L'accoppiamento tra $S^dV^\vee$ e $S^dV$ è una dualità, e $S^d V^\vee$ è isomorfo a $(S^dV)^\vee$.
\end{Cor}
\begin{Lemma}\label{Lemma:gelld}
	Se $g \in S_d$, e $\ell = \sum c_i x_i$, allora
	\[
		g\cdot\ell^d = d!\,g(c_0, \dots, c_n)
	\]
\end{Lemma}
\begin{proof}
	Usando la notazione a multiindice, possiamo notare semplicemente che
	\[
		g\cdot \ell^d = g\cdot\left[\sum_{\abs{\alpha} = d}c^\alpha x^\alpha\binom{d}{\alpha}\right] = \sum_{\abs{\alpha} = d}g_\alpha c^\alpha\alpha!\binom{d}{\alpha}
	\]
	che coincide esattamente con $d!\,g(c_0, \dots c_n)$.
\end{proof}

\begin{Def}
	Dato $f \in S^d V = R_d$, \emph{l'ideale apolare} è definito come
	\[
		f^\bot = \{g \in S \mid g\cdot f = 0\}\,.
	\]
\end{Def}
\begin{Def}
	Una $R$-algebra $S$, con $R$ un anello, è detta Artiniana se $R$ è Artiniano e se $S$ è un $R$-modulo finitamente generato. In particolare, se $R$ è un campo stiamo chiedendo che $\dim_R S$ sia finito.
\end{Def}
\begin{Prop}
	\begin{enumerate}
		\item $f^\bot$ è un ideale omogeneo (i.e.\ è un sottomodulo graduato).
		\item il \emph{zoccolo} $(f^\bot)_d$ ha $K$-codimensione 1.
		\item Se $k > d$, allora $(f^\bot)_k = S^k V$.
		\item L'algebra graduata
		\[
			A_f \deff SV^\vee/(f^\bot) = \bigoplus_{e = 0}^\infty S^eV^\vee/(f^\bot)_e\,.
		\]
		è un'algebra Artiniana.
	\end{enumerate}
\end{Prop}
\begin{Prop}
	Sia $f \in S^d V$. Allora per ogni $e < d$ vale che
	\[
		(f^\bot)_e = [(f^\bot)_e \colon \MM^{d-2}]_e = \{g \in S_e \mid \forall h \in \MM^{d-e}\ (gh)\cdot f = 0\}\,.
	\]
\end{Prop}
\begin{proof}
	L'inclusione $\boxed{\subseteq}$ è immediata.
	
	Per quanto riguarda l'inclusione \boxed{\supseteq}\,, sia $g \in [(f^\bot)_d \colon \MM^{d-e}]$ per cui $(g\partial^\alpha) \cdot f = 0$ per ogni $\abs{\alpha} = d-e$. Siccome $S$ è un anello commutativo, allora
	\[
	(g\partial^\alpha)\cdot f = (\partial^\alpha\,g)\cdot f =  \partial^\alpha\cdot(g\cdot f)\,,
	\]
	e per il Lemma~\ref{Lemma:dualbas}, sappiamo che tutti i coefficienti di $g \cdot f \in S_{d-e}$ sono zero. Conseguentemente, $g \cdot f = 0$, i.e.\ $g$ appartiene a $(f^\bot)_e$.
\end{proof}
\begin{Prop}
	Sia $f \in S^d V$ e sia $e \in \{0,\dots, d\}$. La moltiplicazione 
	\[
		(A_f)_e \times (A_f)_{d-e} \to (A_f)_d \simeq \C
	\]
	è una dualità perfetta.
\end{Prop}
\begin{proof}
	Dimostriamo che la componente di sinistra è non-degenere. Per simmetria lo stesso risultato vale anche per l'altra componente.
	
	Sia $[t] \in (A_f)_e$, tale che $[tu] = 0$ in $(A_f)_f$ per ogni $u \in (A_f)_{d-e}$. In particolare, $tu$ appartiene a $(f^\bot)_d$ per ogni $u \in \MM^{d-e} \subseteq S_{d-e}$. Ergo, $t$ appartiene a $[(f^\bot)_d \colon \MM^{d-e}]_e$. La precedente proposizione implica quindi che $t$ appartiene a $(f^\bot)_e$, i.e.\ $[f] = 0$ in $(A_f)_e$.
\end{proof}

La precedente proposizione ci dice che l'algebra è un'algebra di Gorestein. 

Siano $Z = \{[\ell_1], \dots, [\ell_r]\} \subseteq \PP V$, con $\deg \ell_i = 1$.
\begin{Lemma}[Apolarità]
	$f$ coincide con $\sum \ell_i^d$ se e solo se $I_Z$ è contenuto in $f^\bot$.
\end{Lemma}

Nel caso in cui $\dim_\C V = 2$, allora il lemma precedente diventa come segue.
\begin{Lemma}[Apolarità per forme bilineari]
	Sia $f \in \C[x,y]$, e siano $(\alpha_i \colon \beta_i)$ punti distinti in $\PP^1$, allora
	\[
		\left[\prod_i \beta_i \partial_x - \alpha_i\partial_y\right] \cdot f = 0 \ \Leftrightarrow\ f = \sum_i c_i(\alpha_i x + \beta_i y)^d
	\]
\end{Lemma}
\begin{proof}
	Posto
	\[
		Z = \{[\alpha_1 x + \beta_1 y], \dots, [\alpha_k x + \beta_k y]\}\,,
	\]
	allora il risultato segue dal Lemma di Apolarità una volta provato che
	\[
		I_Z = ((\beta_1\partial_x - \alpha_1\partial_y)\cdot\dots\cdot(\beta_k\partial_x - \alpha_k\partial_y))\,.
	\]
	
	Sicuramente l'ideale proposto, che denotiamo con $J$, è contenuto in $I_Z$.
	
	D'altra parte, sia $g \in I_Z$ omogeneo. Allora $g$ ha $(\alpha_i, \beta_i)$ come radici: per il Lemma~\ref{Lemma:gelld} 
	\begin{equation}\label{eq:ruffini}
		d!g(\alpha_i,\beta_i) = g\cdot (\alpha_i x + \beta_iy) = 0\,.
	\end{equation}
	per ogni $i = 1,\dots, k$. Ma sappiamo che in due variabili vale un teorema di circa-Ruffini. Detto altrimenti, da \eqref{eq:ruffini} sappiamo che il polinomio
	\[
		\prod_{i = 1}^k \beta_1\partial_x - \alpha_1 \partial_y
	\]
	divide $g$.
\end{proof}

Una versione più generale, che introduce delle molteplicità, è data dall'enunciato seguente.
\begin{Prop}
	Sia $f \in \C[x,y]_d$, e siano $(\alpha_i \colon \beta_i)$, $i = 1,\dots, k$, punti di $\PP^1$. Allora
	\[
	\begin{split}
		&\left[\prod_i \beta_i \partial_x - \alpha_i\partial_y\right]\cdot f = 0 \\
		&\Leftrightarrow\ \exists c_i(c,y) \in \C[x,y]_{m_i -1} \text{ t.c. }f = \sum c_i(x,y) (\alpha_i x + \beta_i y)^{d-m_i +1}
	\end{split}
	\]
\end{Prop}
\begin{proof}
	La dimostrazione, in teoria data in classe, è incomprensibile.
\end{proof}

\begin{Def}
		Se $S$ è un anello $\Z_{\ge 0}$-graduato, allora un ideale $I$ \emph{riempe l'anello in grado $k$} se $I$ contiene $S_k$.
\end{Def}
\begin{Lemma}
	Siano $\phi_1$, $\phi_2$ in $\C[x,y] = S$ due polinomi senza fattori in comune.
	\begin{enumerate}
		\item L'ideale $I = (\phi_1, \phi_2)$ riempe l'anello $R$ in gradi maggiori o uguali di $\deg \phi_1 + \deg \phi_2 - 1$. Inoltre, $I$ ha codimensione 1 in grado $d \deff \deg \phi_1 + \deg \phi_2 -1 $.
		\item  $I_e$ coincide con $[I_d \colon \MM^{d-e}]_e$.
	\end{enumerate}
\end{Lemma}
\begin{proof}
	Sia $d_i \deff \deg \phi_i$, e consideriamo la sequenza corta
	\[
	\begin{split}
		0 \to S \to S &\oplus S \to I \to 0\\
		\beta \mapsto (-\beta\phi_2, \beta\phi_1)\\
		&(a,b) \mapsto a\phi_1 + b\phi_2
	\end{split}
	\]
	Siccome $\phi_1$ e $\phi_2$ non hanno fattori in comune, la sequenza precedente è esatta.
	
	Conseguentemente, preso un certo polinomio $g \in S$ di grado $t \ge d_1 + d_2 -1$, allora 
\end{proof}

\begin{Teo}
	L'ideale $f^\bot$ ha sempre due generatori.	
\end{Teo}

\section{Grassmaniane}
Fissiamo un $\C$-spazio vettoriale $V$ di dimensione $n+1$.
\begin{Def}
	La Grassmanniana $\Gr(\PP^k, \PP^n)$, i.e.\ la varietà dei sottospazi di $V$ di dimensione $k+1$, viene identificata con i prodotti wedge indecomponibili in $\Lambda^{k+1} V$.
\end{Def}
Ricordiamo il risultato noto.
\begin{Prop}
	La varietà $\Gr(\PP^k, \PP^n)$ ha la struttura di varietà liscia, ed ha dimensione $(k+1)(n-k)$.
\end{Prop}

Fissiamo una base $e_0, \dots, e_n$ per $V$, in modo da avere una identificazione canonica $V \simeq \C^{n+1}$.

Consideriamo un sottospazio $L = \ang{v_0, \dots, v_k}$ di dimensione $k+1$, ed organizziamo i vettori in righe incolonnate, via una matrice $M_L$ in $M_{(k+1)\times (n+1)}(\C)$ di rango massimo. A questo punto, possiamo osservare il seguente fatto: \emph{due embedding $\C^{k+1} \xrightarrow{i,\,j} V$ danno la stessa immagine se e solo se esiste una trasformazione $g \in GL_{k+1}\C$ per cui $i = jg$}.

Conseguentemente, moltiplicare la matrice $M_L$ a sinistra per un elemento di $GL_{k+1}(\C)$ non cambia lo spazio generato dalle righe (trasposte). Ma questa operazione coincide con una riduzione di Gauss per righe, che porta $M_L$ in 
\[
\begin{bmatrix}
	1 & * & * & 0 & \cdots \\
	0 & 0 & 0 & 1 & \cdots \\
	0 & 0 & 0 & 0 & \cdots\\
	\vdots & \vdots & \vdots & \vdots & \ddots 
\end{bmatrix}
\]

In base alle possibili configurazione di questa forma, possiamo decomporre la Grassmaniana in celle affini, dette \emph{celle di Schubert}.ù

Un modo per organizzare queste celle, in modo da avere una descrizione combinatorica, è tramite le \emph{tabelle di Young}.
\begin{Def}
	La tabella di Young di parametri interi $(\lambda_1 \ge \dots \ge \lambda_{k+1})$ è data dal diagramma seguente:
	\[
		\lambda = \underset{\vdots\ \ \ \ \ \ \ \ \ \ }{\yng(6,4,3)}
	\]
	dove la riga $i$ ha lunghezza $\lambda_i$.
\end{Def}
Per futura utilità definiamo l'insieme delle tabelle di Young
\[
	Y_{k,n} \deff \{\lambda \mid k \text{ righe ed al più } n - k \text{ colonne}\}\,.
\]

Le tabelle di Young possiedono una naturale struttura ad albero, governata dalla relazione di inclusione delle une dentro le altre. Per esempio l'insieme $Y_{1,3}$ si ordine come segue:\vspace{0.3cm}
\begin{figure}[h]
\centering
\begin{tikzpicture}[x=0.75pt, y= 0.75pt]
	\node at (0,40) {$\yng(2)$};
	\node at (0,-40) {$\yng(1,1)$};
	\node at (-100,0) {$\yng(1)$};
	\node at (100,0) {$\yng(2,1)$};
	\node at (-190,0) [font = \Large]{$\varnothing$};
	\node at (200,0) {$\yng(2,2)$};
	\draw (-170,0) -- (-120,0);
	\draw (130,0) -- (170,0);
	\draw (-80,0) -- (-30,30);
	\draw (-80,0) -- (-30,-30);
	\draw (30,30) -- (70,0);
	\draw (30,-30) -- (70,0);
\end{tikzpicture}
\caption{Tabelle di Young con al più 2 righe}
\label{fig:young2}
\end{figure}
\begin{Def}
Fissiamo ora una base $\ang{e_0, \dots, e_n}$ su $V$, e fissiamo la filtrazione
\[
	F_0 \deff \{0\},\ F_i \deff \ang{e_{n-i+1}, \dots, e_n}\,.
\] 
Data una tabella di Young $\lambda \in Y_{k,n}$, la cella $X_\lambda$ è definita come
\[
	X_\lambda \deff \{L \in \Gr_{k+1}(V) \mid \dim(L \cap F_{n-k+i-\lambda_i}) \ge i\ \forall 1 \le i \le k+1\}\,.
\]
\end{Def}
Essa è una \emph{cella affine} di codimensione 
\[
	\abs{\lambda} \deff \#\{\text{scatole in }\lambda\}\,.
\]
In particolare, la cella $C_{(0,\dots, 0)}$ ha dimensione massima. Se guardiamo a alla varietà di Grassmann come varietà algebrica con topologia di Zarinsky, $C_{(0,\dots, 0)}$ è ancora aperto. La varietà di Grassmann è quindi una varietà algebrica particolare: possiede un sottoinsieme aperto, e quindi Zarinskyi-denso, che è anche una sottovarietà algebrica affine.

L'ordinamento delle tabelle di Young fornisce un ordinamento nelle celle della forma
\[
	\lambda \subseteq \mu \ \Rightarrow\ X_\lambda \supseteq X_\mu
\]
Inoltre, questo ordinamento è strettamente legato alla forma di Gauss su righe ridotta: infatti, se andiamo a considerare le differenze
\[
	X_\lambda^0 \deff X_\lambda \setminus \bigcup_{\lambda \subseteq \mu} X_\mu,
\]
allora i differenti $X_\lambda^0$ raggruppano i diversi modi in cui si può presentare una forma di Gauss ridotta su righe di una matrice $(k+1)\times(n+1)$ di rango massimo. 

Per dare maggiore chiarezza al discorso --- per quanto possibile --- consideriamo il caso di $\Gr(\PP^1, \PP^3)$. Essa è la varietà dei 2-sottospazi in $\C^4$, ed ha dimensione complessa 4. Le tabelle di Young $Y_{1,3}$ sono mostrate nella Figura~\ref{fig:young2}. Le relative celle di Schubert $\{X_\lambda\}_\lambda$ sono quindi ordinate come segue:
\begin{center}
\begin{tikzpicture}[x=0.75pt, y=0.75pt]
	\node at (0,40) {$X_{(2,0)}$};
	\node at (0,-40) {$X_{(1,1)}$};
	\node at (-100,0) {$X_{(1,0)}$};
	\node at (100,0) {$X_{(2,1)}$};
	\node at (-190,0) {$X_{(0,0)}$};
	\node at (200,0) {$X_{(2,2)}$};
	\draw (-170,0) -- (-120,0);
	\draw (130,0) -- (170,0);
	\draw (-80,0) -- (-30,30);
	\draw (-80,0) -- (-30,-30);
	\draw (30,30) -- (70,0);
	\draw (30,-30) -- (70,0);
\end{tikzpicture}
\end{center}

In particolare, otteniamo un analogo diagramma se sostituiamo $X_\lambda$ con $X_\lambda^0$. Quest'ultimo corrisponde alle possibili forme di Gauss ridotte per righe tramite il diagramma seguente:
\begin{center}
\begin{tikzpicture}[x=0.75pt, y=0.75pt]
	\node at (0,40) {$
	\begin{bmatrix}
	1 & * & * & 0 \\
	0 & 0 & 0 & 1 	
	\end{bmatrix}
	$};
	\node at (0,-40) {$
	\begin{bmatrix}
	0 & 1 & 0 & * \\
	0 & 0 & 1 & * 	
	\end{bmatrix}
	$};
	\node at (-100,0) {$
	\begin{bmatrix}
	1 & * & 0 & * \\
	0 & 0 & 1 & * 	
	\end{bmatrix}
	$};
	\node at (100,0) {$
	\begin{bmatrix}
	0 & 1 & * & 0 \\
	0 & 0 & 0 & 1 	
	\end{bmatrix}
	$};
	\node at (-210,0) {$
	\begin{bmatrix}
	1 & 0 & * & * \\
	0 & 1 & * & * 	
	\end{bmatrix}
	$};
	\node at (210,0) {$
	\begin{bmatrix}
	0 & 0 & 1 & 0 \\
	0 & 0 & 0 & 1 	
	\end{bmatrix}
	$};
	\draw (-165,0) -- (-145,0);
	\draw (145,0) -- (165,0);
	\draw (-80,30) -- (-45,40);
	\draw (-80,-30) -- (-45,-40);
	\draw (80,30) -- (45,40);
	\draw (80,-30) -- (45,-40);
\end{tikzpicture}
\end{center}

Possiamo effettivamente visualizzare il collegamento tra forma di Gauss ridotta per righe e tabelle di Young. Supponiamo di voler considerare le celle di Schubert nello spazio $\Gr(\PP^k, \PP^n)$. Le relative tabelle di Young appartengono a $Y_{k,n}$, e sono quindi tabelle contenute in una griglia $A_{k,n}$ di forma $(k+1)\times (n-k)$. Una specifica forma di Gauss ridotta avrà degli $*$, che procediamo ad allineare sul lato destro di $A_{k,n}$. Il complementare, una volta riflessa sull'asse orizzontale, sarò la relativa la tabella di Young. 

Per esempio, consideriamo la forma di Gauss in $\Gr(\PP^2, \PP^6)$
\[
	\begin{bmatrix}
		0 & 1 & * & 0 & * & * & 0 \\
		0 & 0 & 0 & 1 & * & * & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 & 1
	\end{bmatrix}	
\]
La matrice $A_{2,6}$ viene riempita come segue:
\begin{equation}\label{eq:Gauss}
	\young(\hfil ***,\hfil\hfil **,\hfil\hfil\hfil\hfil)
\end{equation}
e quindi la tabella di Young associata a \eqref{eq:Gauss} è $\tiny \yng(4,2,1)$.\vspace{1ex}

Parliamo ora della \emph{coomologia della Grassmanniana}. La sua descrizione è puramente combinatorica.
\begin{Teo}
	La coomologia di $\Gr(\PP^k_\C, \PP^n_\C)$ è zero in dimensione dispari, ed è
	\[
		\dim_\R\,H^{2i}(\Gr(\PP^k_\C, \PP^n_\C); \R) = \abs{\{\lambda \in Y_{k,n}\mid \lambda \text{ ha $2i$ scatole}\}}
	\]
	in dimensione pari.
\end{Teo}
\begin{proof}[Sketch della dimostrazione]
	Si può dimostrare che ognuna delle celle di Schubert $X^0_\lambda$ è topologicamente omeomorfa all'interno di un disco complesso $D_\C^i$, con $\abs{\lambda} = i$. In particolare, $X^0_\lambda$ è omeomorfa al disco reale $D_\R^{2i}$. Inoltre, se definiamo
	\[
		X^{(2i)} \deff \bigcup_{\abs{\lambda} = 2i} \overline{X^0_\lambda},
	\]
	allora abbiamo una decomposizione $CW$
	\[	
		X^{(0)} \subseteq \dots \subseteq X^{(2d)},\ d = \dim_\C \Gr(\PP^k, \PP^n)
	\]
	con \emph{celle solo $\R$-dimensione pari}. Ma a questo punto sappiamo il complesso cellulare omologico su $\Z$ è della forma
	\[
		0 \to \Z^{a_0} \overset{\partial}{\to} 0 \overset{\partial}{\to} \Z^{a_2} \overset{\partial}{\to} 0 \overset{\partial}{\to} \dots \overset{\partial}{\to} 0 \overset{\partial}{\to} \Z^{a_{2d}} \to 0 
	\]
	con 
	\[
		a_{2i} = \abs{\{\lambda \in Y_{k,n} \mid \abs{\lambda} = 2i\}}\,.
	\]
	Conseguentemente 
	\[
		H_{2i}(\Gr(\PP^k, \PP^n);\Z) = \Z^{a_{2i}}
	\]
	ed è nulla altrimenti. Per coefficienti univesali concludiamo:
	\[
		H^{2i}(\Gr(\PP^k_\C, \PP^n_\C); \R) = \Hom_\Z(H_{2i}(\Gr(\PP^k, \PP^n);\Z),\R) = \R^{a_{2i}}\,. \qedhere
	\]
\end{proof}
Per il teorema dei coefficienti universali, le dimensioni di 
\[
	\dim_\C\,H^{2i}(\Gr(\PP^k_\C, \PP^n_\C); \C)
\]
con quelle precedenti.

Per esempio
\[
	H^i(\Gr(\PP^1, \PP^3);\C) = 
	\begin{cases}
		\C & i = 0,2,6,8\\
		\C^2 & i = 4\\
		0 &\text{altrimenti}
	\end{cases}
\]
\end{document}